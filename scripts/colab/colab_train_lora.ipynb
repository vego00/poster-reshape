{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # CUDA memory tweaks\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    args = argparse.Namespace(\n",
    "        data_dir='data',\n",
    "        output_dir='./models/lora-checkpoints',\n",
    "        model_id='runwayml/stable-diffusion-v1-5',\n",
    "        lora_rank=4,\n",
    "        num_epochs=10,\n",
    "        batch_size=1,\n",
    "        image_size=512,\n",
    "        max_pairs=60,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    print(\"초기화 시작...\")\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    print(f\"사용 장치: {device}\")\n",
    "\n",
    "    # 모델 & 파이프라인 로드 (기본 float32)\n",
    "    print(\"모델 로딩 중...\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        args.model_id,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.vae.to(device)\n",
    "        pipe.text_encoder.to(device)\n",
    "        pipe.unet.to(device)\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"\\n초기 메모리 사용량:\")\n",
    "        print(f\"- UNet: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
    "\n",
    "    print(\"모델 로딩 완료\")\n",
    "\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    unet = pipe.unet\n",
    "\n",
    "    # LoRA 설정\n",
    "    print(\"LoRA 설정 중...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=4,\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    unet = get_peft_model(unet, lora_config)\n",
    "    print(\"LoRA 설정 완료\")\n",
    "\n",
    "    # 데이터셋 & 데이터로더\n",
    "    print(\"데이터셋 생성 중...\")\n",
    "    dataset = PosterPairDataset(args.data_dir, tokenizer, args.image_size, args.max_pairs)\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "    print(f\"배치 크기: {args.batch_size}\")\n",
    "\n",
    "    # 옵티마이저\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-5)\n",
    "\n",
    "    # 학습 루프\n",
    "    print(\"학습 시작...\")\n",
    "    unet.train()\n",
    "    start_time = time.time()\n",
    "    print_interval = 100\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\n학습 시작 전 메모리 사용량:\")\n",
    "        print(f\"- 할당된 메모리: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
    "        print(f\"- 캐시된 메모리: {torch.cuda.memory_reserved() / 1024**2:.1f}MB\")\n",
    "        print(f\"- 배치 크기 {args.batch_size}로 학습을 시작합니다.\")\n",
    "\n",
    "    total_batches = len(train_dataloader) * args.num_epochs\n",
    "    print(f\"\\n총 배치 수: {total_batches}, 총 에폭: {args.num_epochs}\")\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            target_pixel_values = batch[\"target_pixel_values\"].to(device)\n",
    "            input_ids = batch[\"prompt_ids\"].to(device)\n",
    "\n",
    "            # VAE 인코딩 (horizontal 이미지)\n",
    "            with torch.no_grad():\n",
    "                latents = pipe.vae.encode(target_pixel_values).latent_dist.sample()\n",
    "                latents = latents * pipe.vae.config.scaling_factor\n",
    "                latents = latents.to(device)\n",
    "\n",
    "            timesteps = torch.randint(\n",
    "                0, pipe.scheduler.config.num_train_timesteps,\n",
    "                (latents.shape[0],), device=device\n",
    "            )\n",
    "            noise = torch.randn_like(latents)\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            text_embeddings = text_encoder(input_ids)[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_pred = unet(noisy_latents, timesteps, text_embeddings).sample\n",
    "            loss = F.mse_loss(model_pred, noise)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # NaN 체크\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss at step {step}, breaking.\")\n",
    "                return\n",
    "\n",
    "            # 로그 출력\n",
    "            if step % print_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                ips = (step + 1) / elapsed\n",
    "                remaining = (total_batches - (step + 1)) / ips\n",
    "                print(f\"\\nStep {step}/{total_batches} - Loss: {loss.item():.4f}\")\n",
    "                print(f\"  경과: {timedelta(seconds=int(elapsed))}, 남은: {timedelta(seconds=int(remaining))}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"  GPU 메모리: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
    "\n",
    "    # LoRA 가중치 저장\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    unet.save_pretrained(args.output_dir)\n",
    "    print(f\"LoRA weights saved to {args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기화 시작...\n",
      "사용 장치: cuda\n",
      "모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "초기 메모리 사용량:\n",
      "- UNet: 4133.7MB\n",
      "모델 로딩 완료\n",
      "LoRA 설정 중...\n",
      "LoRA 설정 완료\n",
      "데이터셋 생성 중...\n",
      "데이터셋 로딩 시작: ../../data/pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "영화 폴더 로딩:  87%|████████▋ | 39/45 [00:00<00:00, 5239.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로딩 완료: 60개 페어\n",
      "데이터셋 크기: 60\n",
      "배치 크기: 4\n",
      "학습 시작...\n",
      "\n",
      "학습 시작 전 메모리 사용량:\n",
      "- 할당된 메모리: 4135.2MB\n",
      "- 캐시된 메모리: 4184.0MB\n",
      "- 배치 크기 4로 학습을 시작합니다.\n",
      "\n",
      "학습 정보:\n",
      "- 전체 데이터 수: 60\n",
      "- 배치 크기: 4\n",
      "- 총 배치 수: 150\n",
      "- 총 에폭 수: 10\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. 4.80 GiB allowed; Of the allocated memory 4.67 GiB is allocated by PyTorch, and 45.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    195\u001b[39m text_embeddings = text_encoder(input_ids)[\u001b[32m0\u001b[39m]\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# VAE 인코딩 -> latents\u001b[39;00m\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# pixel_values_cpu = pixel_values.cpu()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     latents = \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m.latent_dist.sample()\n\u001b[32m    201\u001b[39m     latents = latents * pipe.vae.config.scaling_factor\n\u001b[32m    202\u001b[39m     latents = latents.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[39m, in \u001b[36mapply_forward_hook.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_hf_hook\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._hf_hook, \u001b[33m\"\u001b[39m\u001b[33mpre_forward\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m._hf_hook.pre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:278\u001b[39m, in \u001b[36mAutoencoderKL.encode\u001b[39m\u001b[34m(self, x, return_dict)\u001b[39m\n\u001b[32m    276\u001b[39m     h = torch.cat(encoded_slices)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m posterior = DiagonalGaussianDistribution(h)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:252\u001b[39m, in \u001b[36mAutoencoderKL._encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_tiling \u001b[38;5;129;01mand\u001b[39;00m (width > \u001b[38;5;28mself\u001b[39m.tile_sample_min_size \u001b[38;5;129;01mor\u001b[39;00m height > \u001b[38;5;28mself\u001b[39m.tile_sample_min_size):\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tiled_encode(x)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m enc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    254\u001b[39m     enc = \u001b[38;5;28mself\u001b[39m.quant_conv(enc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/models/autoencoders/vae.py:168\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, sample)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# down\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m         sample = \u001b[43mdown_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[32m    171\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.mid_block(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py:1442\u001b[39m, in \u001b[36mDownEncoderBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, *args, **kwargs)\u001b[39m\n\u001b[32m   1439\u001b[39m     deprecate(\u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m\"\u001b[39m, deprecation_message)\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resnets:\n\u001b[32m-> \u001b[39m\u001b[32m1442\u001b[39m     hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m downsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsamplers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/models/resnet.py:328\u001b[39m, in \u001b[36mResnetBlock2D.forward\u001b[39m\u001b[34m(self, input_tensor, temb, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m hidden_states = input_tensor\n\u001b[32m    327\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;66;03m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hidden_states.shape[\u001b[32m0\u001b[39m] >= \u001b[32m64\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:432\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2380\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   2379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. 4.80 GiB allowed; Of the allocated memory 4.67 GiB is allocated by PyTorch, and 45.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colab_train_lora.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
