{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosterPairDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, image_size=512, max_pairs=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_size = image_size\n",
    "        self.pairs = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),                                    # outputs [C=3,H,W] in [0,1]\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),   # standard SD normalization\n",
    "        ])\n",
    "\n",
    "        print(f\"데이터셋 로딩 시작: {data_dir}\")\n",
    "        \n",
    "        # data/pairs 디렉토리에서 모든 영화 폴더를 순회\n",
    "        for movie_name in tqdm(os.listdir(data_dir), desc=\"영화 폴더 로딩\"):\n",
    "            movie_dir = os.path.join(data_dir, movie_name)\n",
    "            if not os.path.isdir(movie_dir):\n",
    "                continue\n",
    "                \n",
    "            # 각 영화 폴더의 pair_* 폴더들을 순회\n",
    "            for pair_dir in os.listdir(movie_dir):\n",
    "                if not pair_dir.startswith('pair_'):\n",
    "                    continue\n",
    "                    \n",
    "                pair_path = os.path.join(movie_dir, pair_dir)\n",
    "                vertical_path = os.path.join(pair_path, 'vertical.jpg')\n",
    "                horizontal_path = os.path.join(pair_path, 'horizontal.jpg')\n",
    "                \n",
    "                if os.path.exists(vertical_path) and os.path.exists(horizontal_path):\n",
    "                    self.pairs.append({\n",
    "                        'vertical': vertical_path,\n",
    "                        'horizontal': horizontal_path,\n",
    "                        'movie': movie_name\n",
    "                    })\n",
    "                    \n",
    "                    # max_pairs가 설정되어 있고, 해당 개수에 도달하면 중단\n",
    "                    if max_pairs and len(self.pairs) >= max_pairs:\n",
    "                        break\n",
    "            if max_pairs and len(self.pairs) >= max_pairs:\n",
    "                break\n",
    "\n",
    "        print(f\"데이터셋 로딩 완료: {len(self.pairs)}개 페어\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # 이미지 로드 및 전처리\n",
    "        with Image.open(pair['vertical']) as vertical_img:\n",
    "            vertical_img = vertical_img.convert('RGB').resize((self.image_size, self.image_size))\n",
    "            pixel_values = self.transform(vertical_img)\n",
    "            \n",
    "        with Image.open(pair['horizontal']) as horizontal_img:\n",
    "            horizontal_img = horizontal_img.convert('RGB').resize((self.image_size, self.image_size))\n",
    "            target_pixel_values = self.transform(horizontal_img)\n",
    "        \n",
    "        # 채널 수를 4로 확장 (RGB + alpha)\n",
    "        # pixel_values = torch.cat([pixel_values, torch.ones_like(pixel_values[:1])], dim=0)\n",
    "        # target_pixel_values = torch.cat([target_pixel_values, torch.ones_like(target_pixel_values[:1])], dim=0)\n",
    "        pixel_values = pixel_values\n",
    "        target_pixel_values = target_pixel_values\n",
    "        \n",
    "        # 프롬프트 생성 및 토큰화\n",
    "        prompt = f\"horizontal movie poster, {pair['movie']}\"\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77)[\"input_ids\"][0]\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"target_pixel_values\": target_pixel_values\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # CUDA 메모리 설정\n",
    "    if torch.cuda.is_available():\n",
    "        # 메모리 단편화 방지\n",
    "        torch.cuda.set_per_process_memory_fraction(0.8)  # 60%로 제한\n",
    "        # 메모리 할당자 설정\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "        # 캐시 비우기\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    args = argparse.Namespace(\n",
    "        data_dir='data',\n",
    "        output_dir='./models/lora-checkpoints',\n",
    "        model_id='runwayml/stable-diffusion-v1-5',\n",
    "        lora_rank=4,\n",
    "        max_train_steps=1000,\n",
    "        batch_size=4,\n",
    "        image_size=512,\n",
    "        max_pairs=60,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    print(\"초기화 시작...\")\n",
    "    \n",
    "    # 가속기 초기화\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    print(f\"사용 장치: {device}\")\n",
    "\n",
    "    # 모델 로드\n",
    "    print(\"모델 로딩 중...\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        args.model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    \n",
    "    # 메모리 최적화를 위한 설정\n",
    "    if torch.cuda.is_available():\n",
    "        pipe.vae.to(device)\n",
    "        pipe.text_encoder.to(device)\n",
    "        pipe.unet.to(device)\n",
    "        \n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        # 메모리 사용량 출력\n",
    "        print(\"\\n초기 메모리 사용량:\")\n",
    "        print(f\"- UNet: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
    "        \n",
    "    print(\"모델 로딩 완료\")\n",
    "\n",
    "    # 토크나이저, 텍스트 인코더\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    unet = pipe.unet\n",
    "\n",
    "    # LoRA 설정\n",
    "    print(\"LoRA 설정 중...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=4,  # alpha 값 감소\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    unet = get_peft_model(unet, lora_config)\n",
    "    print(\"LoRA 설정 완료\")\n",
    "\n",
    "    # 모델을 float16으로 설정\n",
    "    unet = unet.to(dtype=torch.float16)\n",
    "    text_encoder = text_encoder.to(dtype=torch.float16)\n",
    "\n",
    "    # 데이터셋 및 데이터로더 생성\n",
    "    print(\"데이터셋 생성 중...\")\n",
    "    dataset = PosterPairDataset(args.data_dir, tokenizer, args.image_size, args.max_pairs)\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "    print(f\"배치 크기: {args.batch_size}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "\n",
    "    # AMP 스케일러 설정\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # 학습 루프\n",
    "    print(\"학습 시작...\")\n",
    "    unet.train()\n",
    "    start_time = time.time()\n",
    "    last_print_time = start_time\n",
    "    print_interval = 100\n",
    "    \n",
    "    # 학습 시작 전 메모리 사용량 출력\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\n학습 시작 전 메모리 사용량:\")\n",
    "        print(f\"- 할당된 메모리: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
    "        print(f\"- 캐시된 메모리: {torch.cuda.memory_reserved() / 1024**2:.1f}MB\")\n",
    "        print(f\"- 배치 크기 {args.batch_size}로 학습을 시작합니다.\")\n",
    "\n",
    "    # 전체 예상 시간 계산\n",
    "    total_samples = len(dataset)\n",
    "    total_batches = min(args.max_train_steps, total_samples // args.batch_size)\n",
    "    num_epochs = args.max_train_steps // len(train_dataloader) + 1 # Calculate epochs\n",
    "    print(f\"\\n학습 정보:\")\n",
    "    print(f\"- 전체 데이터 수: {total_samples}\")\n",
    "    print(f\"- 배치 크기: {args.batch_size}\")\n",
    "    print(f\"- 총 배치 수: {total_batches}\")\n",
    "    print(f\"- 총 스텝 수: {args.max_train_steps}\")\n",
    "    print(f\"- 총 에폭 수: {num_epochs}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, total=args.max_train_steps)):\n",
    "            # if step >= args.max_train_steps:\n",
    "            #     break\n",
    "            if (epoch * len(train_dataloader) + step) >= args.max_train_steps: \n",
    "                break\n",
    "\n",
    "            # 메모리 정리\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float16)\n",
    "            input_ids = batch[\"prompt_ids\"].to(device)\n",
    "            target_pixel_values = batch[\"target_pixel_values\"].to(device, dtype=torch.float16)\n",
    "            \n",
    "            # 텍스트 임베딩 생성\n",
    "            # with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            text_embeddings = text_encoder(input_ids)[0]\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # VAE 인코딩 -> latents\n",
    "                # pixel_values_cpu = pixel_values.cpu()\n",
    "                latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents * pipe.vae.config.scaling_factor\n",
    "                latents = latents.to(device)\n",
    "\n",
    "            # 노이즈 생성\n",
    "            # timesteps = torch.randint(0, 1000, (pixel_values.shape[0],), device=device).long()\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "            noise = torch.randn_like(latents)\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # 노이즈가 추가된 이미지\n",
    "            # timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "            # noisy_images = pipe.scheduler.add_noise(pixel_values, noise, timesteps)\n",
    "            \n",
    "            # UNet forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "              model_pred = unet(noisy_latents, timesteps, text_embeddings).sample\n",
    "              loss = F.mse_loss(model_pred, noise)\n",
    "            \n",
    "            # AMP를 사용한 역전파\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 100 스텝마다 진행 상황 출력\n",
    "            if step % print_interval == 0:\n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - start_time\n",
    "                steps_per_second = (step + 1) / elapsed_time\n",
    "                remaining_steps = args.max_train_steps - (step + 1)\n",
    "                estimated_remaining_time = remaining_steps / steps_per_second\n",
    "                \n",
    "                print(f\"\\nStep {step}/{args.max_train_steps} - Loss: {loss.item():.4f}\")\n",
    "                print(f\"  경과 시간: {timedelta(seconds=int(elapsed_time))}\")\n",
    "                print(f\"  스텝/초: {steps_per_second:.2f}\")\n",
    "                print(f\"  예상 남은 시간: {timedelta(seconds=int(estimated_remaining_time))}\")\n",
    "                print(f\"  예상 완료 시간: {timedelta(seconds=int(current_time + estimated_remaining_time))}\")\n",
    "                print(f\"  진행률: {(step + 1) / args.max_train_steps * 100:.1f}%\")\n",
    "                \n",
    "                # 메모리 사용량 출력\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"  GPU 메모리: {torch.cuda.memory_allocated() / 1024**2:.1f}MB\")\n",
    "                    torch.cuda.empty_cache()  # 메모리 정리\n",
    "\n",
    "    # LoRA 가중치 저장\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    unet.save_pretrained(args.output_dir)\n",
    "    print(f\"LoRA weights saved to {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기화 시작...\n",
      "사용 장치: cuda\n",
      "모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.14it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. 4.80 GiB allowed; Of the allocated memory 4.74 GiB is allocated by PyTorch, and 55.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m pipe.vae.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m pipe.text_encoder.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m pipe.enable_xformers_memory_efficient_attention()\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# 메모리 사용량 출력\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/diffusers/models/modeling_utils.py:1353\u001b[39m, in \u001b[36mModelMixin.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1348\u001b[39m     logger.warning(\n\u001b[32m   1349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is group offloaded and moving it using `.to()` is not supported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m     )\n\u001b[32m   1351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 915 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/poster-reshape/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. 4.80 GiB allowed; Of the allocated memory 4.74 GiB is allocated by PyTorch, and 55.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colab_train_lora.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
